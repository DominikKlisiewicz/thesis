{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d089db6e-ce46-4879-8c44-4e64bff90bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# Logging\n",
    "# =========================\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# =========================\n",
    "# Point-2 Game Logic\n",
    "# =========================\n",
    "\n",
    "class TicTacToeBoard:\n",
    "    PLAYER_MAP = {1: \"X\", -1: \"O\"}\n",
    "    STATE_MAP = {1: \"unfinished\", 2: \"draw\", 3: \"finished\"}\n",
    "    SECTION_MAP = {\n",
    "        1: [0, 0],\n",
    "        2: [0, 3],\n",
    "        3: [0, 6],\n",
    "        4: [3, 0],\n",
    "        5: [3, 3],\n",
    "        6: [3, 6],\n",
    "        7: [6, 0],\n",
    "        8: [6, 3],\n",
    "        9: [6, 6],\n",
    "        10: [0, 0]\n",
    "    }\n",
    "    SECTION_WON_REWARD = 20  # Reward for winning a section\n",
    "    SECTION_DRAW_REWARD = 5  # Reward for drawing a section\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset_board()\n",
    "        \n",
    "    def reset_board(self):\n",
    "        \"\"\"Reset all game boards to their initial state\"\"\"\n",
    "        self.grid = np.zeros((9, 9), dtype=np.int8)\n",
    "        self.section_grid = np.zeros((3, 3), dtype=np.int8)\n",
    "        self.section_state_grid = np.ones((3, 3), dtype=bool)\n",
    "        self.possible_moves_grid = np.ones((9, 9), dtype=bool)\n",
    "\n",
    "        self.possible_moves = np.ones((9, 9), dtype=bool)\n",
    "\n",
    "        self.grid_filter = np.ones((9, 9), dtype=bool)\n",
    "        self.current_player = 1\n",
    "        self.current_reward = 0\n",
    "    \n",
    "    def print_board(self, grid: np.ndarray = None):\n",
    "        \"\"\"Print the game board with symbols\"\"\"\n",
    "        if grid is None:\n",
    "            grid = self.grid\n",
    "            \n",
    "        symbols = {0: '·', 1: 'x', -1: 'o'}\n",
    "        for row in grid:\n",
    "            print(\" \".join([symbols[x] for x in row]))\n",
    "    \n",
    "    def print_possible_moves_board(self, grid: np.ndarray = None):\n",
    "        \"\"\"Print the possible moves board with symbols\"\"\"\n",
    "        if grid is None:\n",
    "            grid = self.possible_moves_grid\n",
    "            \n",
    "        symbols = {False: '❌', True: '❎'}\n",
    "        for row in grid:\n",
    "            print(\" \".join([symbols[x] for x in row]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_row_col(index: int) -> tuple[int, int]:\n",
    "        \"\"\"Convert linear index to row and column\"\"\"\n",
    "        row = index // 9\n",
    "        col = index % 9\n",
    "        return (row, col)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_section(row: int, col: int) -> int:\n",
    "        \"\"\"Get section number from row and column\"\"\"\n",
    "        return (row // 3) * 3 + (col // 3) + 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_local_index(row: int, col: int) -> int:\n",
    "        \"\"\"Get local index within a section\"\"\"\n",
    "        local_row = row % 3\n",
    "        local_col = col % 3\n",
    "        return local_row * 3 + local_col + 1\n",
    "    \n",
    "    def map_section(self, section_number: int) -> list[int]:\n",
    "        \"\"\"Get starting indices for a section\"\"\"\n",
    "        return self.SECTION_MAP.get(section_number, \"Invalid section number\")\n",
    "    \n",
    "    def create_bool_grid_filter(self, section_number: int) -> np.ndarray:\n",
    "        \"\"\"Create a filter for possible moves in a specific section\"\"\"\n",
    "        grid_filter = np.zeros((9, 9), dtype=bool)\n",
    "        section_indices = self.map_section(section_number)\n",
    "        if isinstance(section_indices, str):\n",
    "            return grid_filter\n",
    "        row_start = section_indices[0]\n",
    "        col_start = section_indices[1]\n",
    "        grid_filter[row_start:row_start+3, col_start:col_start+3] = \\\n",
    "            self.possible_moves_grid[row_start:row_start+3, col_start:col_start+3]\n",
    "        return grid_filter\n",
    "    \n",
    "    def move(self, player: int, index: int):\n",
    "        \"\"\"Make a move on the board\"\"\"\n",
    "        row, col = self.get_row_col(index)\n",
    "        self.grid[row, col] = player\n",
    "        self.possible_moves_grid[row, col] = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_diagonal_sums(section: np.ndarray) -> tuple[int, int]:\n",
    "        \"\"\"Calculate sums of both diagonals in a section\"\"\"\n",
    "        main_diag_sum = section[0, 0] + section[1, 1] + section[2, 2]\n",
    "        anti_diag_sum = section[0, 2] + section[1, 1] + section[2, 0]\n",
    "        return (main_diag_sum, anti_diag_sum)\n",
    "    \n",
    "    def extract_section(self, grid: np.ndarray, section_number: int) -> np.ndarray:\n",
    "        \"\"\"Extract a 3x3 section from the main grid\"\"\"\n",
    "        section_indices = self.map_section(section_number)\n",
    "        if isinstance(section_indices, str):\n",
    "            return np.zeros((3, 3), dtype=np.int8)\n",
    "        initial_row_index = section_indices[0]\n",
    "        initial_col_index = section_indices[1]\n",
    "        return grid[initial_row_index:initial_row_index+3, initial_col_index:initial_col_index+3]\n",
    "    \n",
    "    def check_section_state(self, section: np.ndarray) -> int:\n",
    "        \"\"\"Check the state of a section (unfinished, draw, or won)\"\"\"\n",
    "        row_sum = np.abs(np.sum(section, axis=1))\n",
    "        col_sum = np.abs(np.sum(section, axis=0))\n",
    "        diagonal_sum = np.abs(self.get_diagonal_sums(section))\n",
    "        \n",
    "        if 3 in col_sum or 3 in row_sum or 3 in diagonal_sum:\n",
    "            return 3  # Won\n",
    "        elif 0 in section:\n",
    "            return 1   # Unfinished\n",
    "        else:\n",
    "            return 2   # Draw\n",
    "    \n",
    "    def clear_section(self, grid: np.ndarray, section_number: int):\n",
    "        \"\"\"Clear a section in the given grid\"\"\"\n",
    "        row_start = ((section_number - 1) // 3) * 3\n",
    "        col_start = ((section_number - 1) % 3) * 3\n",
    "        grid[row_start:row_start+3, col_start:col_start+3] = False\n",
    "        self.grid[row_start:row_start+3, col_start:col_start+3] = self.current_player\n",
    "    \n",
    "    def update_section_grid(self, section_number: int, value: int):\n",
    "        \"\"\"Update the section grid with a player's win\"\"\"\n",
    "        row_index = (section_number - 1) // 3\n",
    "        col_index = (section_number - 1) % 3\n",
    "        self.section_grid[row_index, col_index] = value\n",
    "    \n",
    "    def update_section_state_grid(self, section_number: int):\n",
    "        \"\"\"Mark a section as completed in the state grid\"\"\"\n",
    "        row_index = (section_number - 1) // 3\n",
    "        col_index = (section_number - 1) % 3\n",
    "        self.section_state_grid[row_index, col_index] = False\n",
    "    \n",
    "    def print_possible_moves(self, is_whole_board_active: bool, section_number: int, print_output = True):\n",
    "        \"\"\"Print the current possible moves\"\"\"\n",
    "        if print_output:\n",
    "            print(\"General state:\")\n",
    "            self.print_possible_moves_board(self.section_state_grid)\n",
    "            print(\"Specific state:\")\n",
    "        if is_whole_board_active:\n",
    "            if print_output:\n",
    "                self.print_possible_moves_board(self.possible_moves_grid)\n",
    "            return self.possible_moves_grid\n",
    "        else:\n",
    "            filter_grid = self.create_bool_grid_filter(section_number)\n",
    "            if print_output:\n",
    "                self.print_possible_moves_board(filter_grid)\n",
    "            return filter_grid\n",
    "        \n",
    "    \n",
    "    def check_if_move_possible(self, is_whole_board_active: bool, index: int, section_number: int) -> bool:\n",
    "        \"\"\"Check if a move is valid\"\"\"\n",
    "        row, col = self.get_row_col(index)\n",
    "        if is_whole_board_active:\n",
    "            return self.possible_moves_grid[row, col]\n",
    "        else:\n",
    "            filter_grid = self.create_bool_grid_filter(section_number)\n",
    "            return filter_grid[row, col]\n",
    "    \n",
    "    def post_move_transformations(self, index: int, player: int) -> tuple[bool, int]:\n",
    "        \"\"\"Update game state after a move\"\"\"\n",
    "        row, col = self.get_row_col(index)\n",
    "        section_number = self.get_section(row, col)\n",
    "        local_index = self.get_local_index(row, col)\n",
    "        section = self.extract_section(self.grid, section_number)\n",
    "        section_state = self.check_section_state(section)\n",
    "\n",
    "        if section_state == 3:  # Section won\n",
    "            self.update_section_grid(section_number, player)\n",
    "            self.update_section_state_grid(section_number)\n",
    "            self.clear_section(self.possible_moves_grid, section_number)\n",
    "            self.current_reward = self.SECTION_WON_REWARD * self.current_player\n",
    "        elif section_state == 2:  # Section draw\n",
    "            self.update_section_state_grid(section_number)\n",
    "            self.clear_section(self.possible_moves_grid, section_number)\n",
    "            self.current_reward = self.SECTION_DRAW_REWARD\n",
    "        else:  # Section unfinished\n",
    "            self.current_reward = -self.current_player\n",
    "\n",
    "        # Determine next active section\n",
    "        local_section_row_index = (local_index - 1) // 3\n",
    "        local_section_col_index = (local_index - 1) % 3\n",
    "        is_next_section_free = self.section_state_grid[local_section_row_index, local_section_col_index]\n",
    "        self.current_player *= -1\n",
    "\n",
    "        if not is_next_section_free:\n",
    "            self.grid_filter = self.possible_moves_grid\n",
    "            return True, 0  # Whole board active\n",
    "        else:\n",
    "            self.grid_filter = self.create_bool_grid_filter(local_index)\n",
    "            return False, local_index  # Specific section active\n",
    "\n",
    "\n",
    "class TicTacToeGame:\n",
    "    def __init__(self):\n",
    "        self.board = TicTacToeBoard()\n",
    "        self.current_player = self.board.current_player\n",
    "        self.is_whole_board_active = True\n",
    "        self.active_section = 0\n",
    "        self.last_move = None\n",
    "        self.state = 0  # 0: ongoing, 1: player 1 won, -1: player 2 won, 2: draw\n",
    "    \n",
    "    def get_user_move(self) -> int:\n",
    "        \"\"\"Get valid move input from user\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                user_move_str = input(\"Enter your move (0-80): \")\n",
    "                index = int(user_move_str)\n",
    "                if 0 <= index <= 80:\n",
    "                    return index\n",
    "                print(\"Invalid move. Please enter a number between 0 and 80.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "    def make_move(self, index):\n",
    "        if self.board.check_if_move_possible(self.is_whole_board_active, index, self.active_section):\n",
    "            self.board.move(self.board.current_player, index)\n",
    "            self.is_whole_board_active, self.active_section = \\\n",
    "                self.board.post_move_transformations(index, self.board.current_player)\n",
    "            self.last_move = index\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def check_winner(self):\n",
    "        # Check if the game is won (3 in a row in the section grid)\n",
    "        section_grid = self.board.section_grid\n",
    "        row_sum = np.abs(np.sum(section_grid, axis=1))\n",
    "        col_sum = np.abs(np.sum(section_grid, axis=0))\n",
    "        diag1 = abs(section_grid[0,0] + section_grid[1,1] + section_grid[2,2])\n",
    "        diag2 = abs(section_grid[0,2] + section_grid[1,1] + section_grid[2,0])\n",
    "        \n",
    "        if 3 in row_sum or 3 in col_sum or diag1 == 3 or diag2 == 3:\n",
    "            self.state = self.board.current_player * -1  # Winner is the player who just moved\n",
    "            return self.state\n",
    "        \n",
    "        # Check if game is a draw (all sections are completed)\n",
    "        if not np.any(self.board.section_state_grid):\n",
    "            self.state = 2\n",
    "            return \"Tie\"\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def get_move_filter(self):\n",
    "        return self.board.grid_filter.flatten().tolist()\n",
    "    \n",
    "    def play(self):\n",
    "        \"\"\"Main game loop (manual play)\"\"\"\n",
    "        while True:\n",
    "            self.board.print_possible_moves(self.is_whole_board_active, self.active_section)\n",
    "            self.board.print_board()\n",
    "            \n",
    "            index = self.get_user_move()\n",
    "            if not self.board.check_if_move_possible(self.is_whole_board_active, index, self.active_section):\n",
    "                print(f\"Move to [{index}] is impossible\")\n",
    "                continue\n",
    "\n",
    "            self.board.move(self.current_player, index)\n",
    "            self.is_whole_board_active, self.active_section = \\\n",
    "                self.board.post_move_transformations(index, self.current_player)\n",
    "\n",
    "            game_section_state = self.board.check_section_state(self.board.section_grid)\n",
    "            if game_section_state == 3:  # Game won\n",
    "                self.board.print_board()\n",
    "                print(f\"Game finished, player {self.board.PLAYER_MAP[self.current_player]} won!\")\n",
    "                break\n",
    "            elif game_section_state == 2:  # Game draw\n",
    "                self.board.print_board()\n",
    "                print(\"Game finished, tie!\")\n",
    "                break\n",
    "\n",
    "            self.current_player *= -1  # Switch player\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RL Environment (wraps point-2)\n",
    "# =========================\n",
    "\n",
    "class UltimateTicTacToeEnv:\n",
    "    \"\"\"RL environment wrapper for Ultimate Tic Tac Toe using point-2 logic.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.game = TicTacToeGame()\n",
    "        self.action_space = 81  # 0-80\n",
    "        self.observation_space = (9, 9)  # Board shape\n",
    "        self.valid_actions = self.get_valid_actions()\n",
    "\n",
    "    def reset(self):\n",
    "        del self.game\n",
    "        self.game = TicTacToeGame()\n",
    "        obs = self._get_obs()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action, force_legal_move=False):\n",
    "        valid_actions = self.get_valid_actions()\n",
    "        if action not in valid_actions:\n",
    "            # Invalid moves are strongly penalized and terminate\n",
    "            obs = self._get_obs()\n",
    "            return obs, -100.0, True, {'winner': -self.game.board.current_player, 'valid': False}\n",
    "\n",
    "        self.valid_actions = valid_actions\n",
    "        self.game.make_move(action)\n",
    "        winner = self.game.check_winner()\n",
    "        obs = self._get_obs()\n",
    "        if winner == 1:\n",
    "            reward = 100.0     # point-2 terminal reward\n",
    "            done = True\n",
    "        elif winner == -1:\n",
    "            reward = -100.0    # point-2 terminal reward\n",
    "            done = True\n",
    "        elif winner == \"Tie\":\n",
    "            reward = 0.5       # point-2 terminal reward\n",
    "            done = True\n",
    "        else:\n",
    "            reward = float(self.game.board.current_reward)  # point-2 per-move/section rewards\n",
    "            done = False\n",
    "        return obs, reward, done, {\"winner\" : winner, \"valid\": True}\n",
    "\n",
    "    def render(self):\n",
    "        self.game.board.print_board()\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        valid_moves = self.game.board.print_possible_moves(self.game.is_whole_board_active, self.game.active_section, False)\n",
    "        flat_mask = valid_moves.flatten()\n",
    "        valid_indices = np.where(flat_mask)[0]\n",
    "        return valid_indices.tolist()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Return a copy of the board state as observation\n",
    "        return (np.copy(self.game.board.grid), np.copy(self.game.board.section_grid), np.copy(self.game.board.grid_filter))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helper: state -> tensor (kept from your original)\n",
    "# =========================\n",
    "\n",
    "def convert_state_to_tensor(state, player):\n",
    "    \"\"\"\n",
    "    Converts a state tuple into a PyTorch tensor with the correct\n",
    "    channel-first format for the network.\n",
    "\n",
    "    Args:\n",
    "        state (tuple): (main_board, section_board, valid_mask)\n",
    "        player (int): 1 or -1, the current player's turn.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (1, 3, 9, 9).\n",
    "    \"\"\"\n",
    "    board, macroboard, valid_mask = state\n",
    "\n",
    "    # Flip perspective so the current player is always '1'\n",
    "    p_board = board * player\n",
    "    p_macroboard = macroboard * player\n",
    "\n",
    "    # Expand 3x3 macro to 9x9\n",
    "    expanded_macro = np.repeat(p_macroboard, 3, axis=0).repeat(3, axis=1)\n",
    "\n",
    "    stacked_state = np.stack(\n",
    "        [p_board, expanded_macro, valid_mask.astype(np.float32)],\n",
    "        axis=0\n",
    "    )\n",
    "    tensor_state = torch.from_numpy(stacked_state).float().unsqueeze(0)\n",
    "    return tensor_state\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Replay Buffer\n",
    "# =========================\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DQN Network\n",
    "# =========================\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Ultimate Tic-Tac-Toe.\"\"\"\n",
    "    def __init__(self, input_shape=(3, 9, 9), n_actions=81):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        c, h, w = input_shape\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(c, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * h * w, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MCTS (wired to env.get_valid_actions + network eval)\n",
    "# =========================\n",
    "\n",
    "class MCTSNode:\n",
    "    \"\"\"A node in the Monte Carlo Tree.\"\"\"\n",
    "    def __init__(self, env_snapshot, parent=None, action=None):\n",
    "        self.env = env_snapshot          # a deep-copied env\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.total_value = 0.0\n",
    "        self.untried_actions = self.env.get_valid_actions()\n",
    "        self.is_terminal = False\n",
    "\n",
    "    def ucb1(self, c_param=1.4):\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        return (self.total_value / self.visits) + c_param * math.sqrt(math.log(self.parent.visits + 1) / self.visits)\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    def __init__(self, agent_network, c_param=1.4):\n",
    "        self.net = agent_network\n",
    "        self.c_param = c_param\n",
    "        self.device = agent_network.device\n",
    "\n",
    "    def search(self, root_env, num_simulations=100):\n",
    "        root = MCTSNode(copy.deepcopy(root_env), parent=None, action=None)\n",
    "\n",
    "        # If no moves, return anything\n",
    "        if not root.untried_actions:\n",
    "            return random.randint(0, 80)\n",
    "\n",
    "        for _ in range(num_simulations):\n",
    "            node = self._select(root)\n",
    "            if not node.is_terminal:\n",
    "                node = self._expand(node)\n",
    "            value = self._evaluate(node)\n",
    "            self._backpropagate(node, value)\n",
    "\n",
    "        # Choose by visit count\n",
    "        best_child = max(root.children, key=lambda c: c.visits) if root.children else None\n",
    "        return best_child.action if best_child else random.choice(root.untried_actions)\n",
    "\n",
    "    def _select(self, node):\n",
    "        # Traverse down by UCB1 until we hit a node we can expand or terminal\n",
    "        while True:\n",
    "            if node.is_terminal or node.untried_actions:\n",
    "                return node\n",
    "            if not node.children:\n",
    "                return node\n",
    "            node = max(node.children, key=lambda ch: ch.ucb1(self.c_param))\n",
    "\n",
    "    def _expand(self, node):\n",
    "        if not node.untried_actions:\n",
    "            return node\n",
    "        action = node.untried_actions.pop()\n",
    "        next_env = copy.deepcopy(node.env)\n",
    "        _, _, done, _ = next_env.step(action)\n",
    "        child = MCTSNode(next_env, parent=node, action=action)\n",
    "        child.is_terminal = done\n",
    "        node.children.append(child)\n",
    "        return child\n",
    "\n",
    "    def _evaluate(self, node):\n",
    "        # If terminal, value is immediate outcome from current env perspective\n",
    "        # Map env rewards to a value: we'll do a single-network value estimate for non-terminal\n",
    "        if node.is_terminal:\n",
    "            # Use a simple terminal scoring consistent with env: winner already baked into env state.\n",
    "            # We'll re-evaluate with network to keep scale consistent.\n",
    "            pass\n",
    "\n",
    "        # Network Q-max over valid moves as a proxy value\n",
    "        with torch.no_grad():\n",
    "            state = node.env._get_obs()\n",
    "            player = node.env.game.board.current_player\n",
    "            state_tensor = convert_state_to_tensor(state, player).to(self.device)\n",
    "            q_values = self.net(state_tensor).squeeze(0).cpu().numpy()\n",
    "\n",
    "            valid = np.full(81, False)\n",
    "            valid[node.env.get_valid_actions()] = True\n",
    "            q_values[~valid] = -1e9\n",
    "            value = float(np.max(q_values))\n",
    "        return value\n",
    "\n",
    "    def _backpropagate(self, node, value):\n",
    "        cur = node\n",
    "        while cur is not None:\n",
    "            cur.visits += 1\n",
    "            cur.total_value += value\n",
    "            value = -value  # alternate perspective each ply\n",
    "            cur = cur.parent\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DQN Agent (keeps your structure)\n",
    "# =========================\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.n_actions = self.env.action_space\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0   # used minimally since we rely on MCTS\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 1e-4\n",
    "        self.target_update_frequency = 1000\n",
    "        \n",
    "        self.q_network = DQNetwork()\n",
    "        self.target_network = DQNetwork()\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(capacity=100000)\n",
    "        self.steps_done = 0\n",
    "        self.mcts = MCTS(self.q_network, c_param=1.4)\n",
    "\n",
    "    def get_action(self, state, player, valid_actions, num_simulations=100):\n",
    "        # Use MCTS, which uses env snapshots and valid moves\n",
    "        return self.mcts.search(self.env, num_simulations)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        experiences = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        # NOTE: Keeping your original structure:\n",
    "        # states viewed as player=1, next_states as player=-1\n",
    "        states_t = torch.cat([convert_state_to_tensor(s, 1) for s in states]).to(self.q_network.device)\n",
    "        actions_t = torch.tensor(actions, dtype=torch.long).unsqueeze(-1).to(self.q_network.device)\n",
    "        rewards_t = torch.tensor(rewards, dtype=torch.float32).to(self.q_network.device)\n",
    "        next_states_t = torch.cat([convert_state_to_tensor(s, -1) for s in next_states]).to(self.q_network.device)\n",
    "        dones_t = torch.tensor(dones, dtype=torch.float32).to(self.q_network.device)\n",
    "        \n",
    "        q_values = self.q_network(states_t).gather(1, actions_t).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states_t).max(1)[0]\n",
    "            target_q_values = rewards_t + self.gamma * next_q_values * (1 - dones_t)\n",
    "            \n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if self.steps_done % self.target_update_frequency == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Training Loop\n",
    "# =========================\n",
    "\n",
    "# def train(agent, env, num_episodes=10000):\n",
    "#     logger.info(\"Starting training...\")\n",
    "#     for episode in tqdm(range(num_episodes)):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0.0\n",
    "        \n",
    "#         while not done:\n",
    "#             # Player 1's turn\n",
    "#             valid_moves_p1 = env.get_valid_actions()\n",
    "#             action_p1 = agent.get_action(state, env.game.board.current_player, valid_moves_p1, num_simulations=200)\n",
    "#             next_state_p1, reward_p1, done, info_p1 = env.step(action_p1)\n",
    "            \n",
    "#             agent.replay_buffer.push(state, action_p1, reward_p1, next_state_p1, done)\n",
    "#             agent.learn()\n",
    "            \n",
    "#             state = next_state_p1\n",
    "#             total_reward += reward_p1\n",
    "#             if done:\n",
    "#                 break\n",
    "                \n",
    "#             # Player -1's turn (self-play)\n",
    "#             valid_moves_p_1 = env.get_valid_actions()\n",
    "#             action_p_1 = agent.get_action(state, env.game.board.current_player, valid_moves_p_1, num_simulations=200)\n",
    "#             next_state_p_1, reward_p_1, done, info_p_1 = env.step(action_p_1)\n",
    "            \n",
    "#             # Zero-sum perspective (keep structure; you used negative here)\n",
    "#             agent.replay_buffer.push(state, action_p_1, -reward_p_1, next_state_p_1, done)\n",
    "#             agent.learn()\n",
    "            \n",
    "#             state = next_state_p_1\n",
    "#             total_reward += reward_p_1\n",
    "#             if done:\n",
    "#                 break\n",
    "        \n",
    "#         agent.update_epsilon()\n",
    "\n",
    "#         # Save periodically\n",
    "#         if (episode + 1) % 500 == 0:\n",
    "#             timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#             filename = f\"ultimate_ttt_agent_episode_{episode+1}_{timestamp}.pt\"\n",
    "#             torch.save(agent.q_network.state_dict(), filename)\n",
    "#             logger.info(f\"Model saved as {filename} (episode {episode+1}, total_reward={total_reward:.2f})\")\n",
    "\n",
    "#     logger.info(\"Training complete.\")\n",
    "#     torch.save(agent.q_network.state_dict(), \"ultimate_ttt_agent_final.pt\")\n",
    "#     logger.info(\"Final model saved as ultimate_ttt_agent_final.pt\")\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def train(agent, env, num_episodes=10000):\n",
    "    logger.info(\"Starting training...\")\n",
    "    \n",
    "    # Ensure save directory exists\n",
    "    SAVE_DIR = \"saved_models\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            # Player 1's turn\n",
    "            valid_moves_p1 = env.get_valid_actions()\n",
    "            action_p1 = agent.get_action(state, env.game.board.current_player, valid_moves_p1, num_simulations=200)\n",
    "            next_state_p1, reward_p1, done, info_p1 = env.step(action_p1)\n",
    "            \n",
    "            agent.replay_buffer.push(state, action_p1, reward_p1, next_state_p1, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            state = next_state_p1\n",
    "            total_reward += reward_p1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            # Player -1's turn (self-play)\n",
    "            valid_moves_p_1 = env.get_valid_actions()\n",
    "            action_p_1 = agent.get_action(state, env.game.board.current_player, valid_moves_p_1, num_simulations=200)\n",
    "            next_state_p_1, reward_p_1, done, info_p_1 = env.step(action_p_1)\n",
    "            \n",
    "            # Zero-sum perspective\n",
    "            agent.replay_buffer.push(state, action_p_1, -reward_p_1, next_state_p_1, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            state = next_state_p_1\n",
    "            total_reward += reward_p_1\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update_epsilon()\n",
    "\n",
    "        # Save every 500 episodes\n",
    "        if (episode + 1) % 500 == 0:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"ultimate_ttt_agent_ep{episode+1}_{timestamp}.pt\"\n",
    "            save_path = os.path.join(SAVE_DIR, filename)\n",
    "            torch.save(agent.q_network.state_dict(), save_path)\n",
    "            logger.info(f\"Model saved at {save_path} (episode {episode+1}, total_reward={total_reward:.2f})\")\n",
    "\n",
    "    logger.info(\"Training complete.\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"ultimate_ttt_agent_ep{episode+1}_{timestamp}.pt\"\n",
    "    final_model_path = os.path.join(SAVE_DIR, filename)\n",
    "    torch.save(agent.q_network.state_dict(), final_model_path)\n",
    "    logger.info(f\"Final model saved at {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "686a84dc-bd4b-4dc8-b9db-9bd2aed17e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting training...\n",
      "  0%|                                                                                            | 0/1 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m agent = DQNAgent(env)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Train (adjust episodes/simulations for speed while testing)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Agent vs. Human Player Test ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 693\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(agent, env, num_episodes)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;66;03m# Player -1's turn (self-play)\u001b[39;00m\n\u001b[32m    692\u001b[39m valid_moves_p_1 = env.get_valid_actions()\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m action_p_1 = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_player\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_moves_p_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    694\u001b[39m next_state_p_1, reward_p_1, done, info_p_1 = env.step(action_p_1)\n\u001b[32m    696\u001b[39m \u001b[38;5;66;03m# Zero-sum perspective\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 572\u001b[39m, in \u001b[36mDQNAgent.get_action\u001b[39m\u001b[34m(self, state, player, valid_actions, num_simulations)\u001b[39m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, player, valid_actions, num_simulations=\u001b[32m100\u001b[39m):\n\u001b[32m    571\u001b[39m     \u001b[38;5;66;03m# Use MCTS, which uses env snapshots and valid moves\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 487\u001b[39m, in \u001b[36mMCTS.search\u001b[39m\u001b[34m(self, root_env, num_simulations)\u001b[39m\n\u001b[32m    485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m node.is_terminal:\n\u001b[32m    486\u001b[39m         node = \u001b[38;5;28mself\u001b[39m._expand(node)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m     \u001b[38;5;28mself\u001b[39m._backpropagate(node, value)\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# Choose by visit count\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 526\u001b[39m, in \u001b[36mMCTS._evaluate\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    524\u001b[39m state = node.env._get_obs()\n\u001b[32m    525\u001b[39m player = node.env.game.board.current_player\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m state_tensor = \u001b[43mconvert_state_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m    527\u001b[39m q_values = \u001b[38;5;28mself\u001b[39m.net(state_tensor).squeeze(\u001b[32m0\u001b[39m).cpu().numpy()\n\u001b[32m    529\u001b[39m valid = np.full(\u001b[32m81\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 391\u001b[39m, in \u001b[36mconvert_state_to_tensor\u001b[39m\u001b[34m(state, player)\u001b[39m\n\u001b[32m    385\u001b[39m expanded_macro = np.repeat(p_macroboard, \u001b[32m3\u001b[39m, axis=\u001b[32m0\u001b[39m).repeat(\u001b[32m3\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    387\u001b[39m stacked_state = np.stack(\n\u001b[32m    388\u001b[39m     [p_board, expanded_macro, valid_mask.astype(np.float32)],\n\u001b[32m    389\u001b[39m     axis=\u001b[32m0\u001b[39m\n\u001b[32m    390\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m tensor_state = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacked_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_state\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Main & Test (human vs agent)\n",
    "# =========================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = UltimateTicTacToeEnv()\n",
    "    agent = DQNAgent(env)\n",
    "\n",
    "    # Train (adjust episodes/simulations for speed while testing)\n",
    "    train(agent, env, num_episodes=1)\n",
    "\n",
    "    logger.info(\"\\n--- Agent vs. Human Player Test ---\")\n",
    "    try:\n",
    "        agent.q_network.load_state_dict(torch.load(\"ultimate_ttt_agent_final.pt\", map_location=agent.q_network.device))\n",
    "        logger.info(\"Loaded final model for testing.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Final model not found. Please run the training first.\")\n",
    "    \n",
    "    num_test_simulations = 400  # increase for stronger MCTS\n",
    "\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    while True:\n",
    "        # Stop if done\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "\n",
    "        if env.game.board.current_player == 1:  # Human is X\n",
    "            try:\n",
    "                move = int(input(\"Enter your move (0-80): \"))\n",
    "                if move not in env.get_valid_actions():\n",
    "                    logger.info(\"Invalid move. Try again.\")\n",
    "                    continue\n",
    "                _, _, done, _ = env.step(move)\n",
    "            except ValueError:\n",
    "                logger.info(\"Invalid input. Please enter a number.\")\n",
    "                continue\n",
    "        else:  # Agent's turn\n",
    "            logger.info(\"Agent is thinking with MCTS...\")\n",
    "            state = env._get_obs()\n",
    "            valid_moves = env.get_valid_actions()\n",
    "            action = agent.get_action(state, env.game.board.current_player, valid_moves, num_simulations=num_test_simulations)\n",
    "            logger.info(f\"Agent plays at: {action}\")\n",
    "            _, _, done, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    final_winner = env.game.check_winner()\n",
    "    if final_winner == 1:\n",
    "        logger.info(\"Congratulations! You won!\")\n",
    "    elif final_winner == -1:\n",
    "        logger.info(\"The agent won. Better luck next time!\")\n",
    "    else:\n",
    "        logger.info(\"It's a draw!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b37d2-ef76-4161-aa0d-fb0da01dcfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
