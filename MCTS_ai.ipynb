{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d862066-9f1c-402d-b2e1-210ca66bb3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Set up logging to avoid printing too much during training\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class UltimateTicTacToeBoard:\n",
    "    \"\"\"\n",
    "    Represents the game state of Ultimate Tic-Tac-Toe.\n",
    "    This class handles all game rules and board manipulations.\n",
    "    \"\"\"\n",
    "    PLAYER_MAP = {1: \"X\", -1: \"O\"}\n",
    "    SECTION_MAP = {\n",
    "        0: (0, 0), 1: (0, 3), 2: (0, 6),\n",
    "        3: (3, 0), 4: (3, 3), 5: (3, 6),\n",
    "        6: (6, 0), 7: (6, 3), 8: (6, 6)\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset all game boards to their initial state.\"\"\"\n",
    "        # The main 9x9 board\n",
    "        self.grid = np.zeros((9, 9), dtype=np.int8)\n",
    "        # The 3x3 board representing which sub-boards are won\n",
    "        self.section_grid = np.zeros((3, 3), dtype=np.int8)\n",
    "        # The active section for the next move (0-8), or -1 if any section is valid\n",
    "        self.active_section = -1\n",
    "        self.current_player = 1\n",
    "        self.last_move = None\n",
    "        self.game_over = False\n",
    "        self.winner = 0  # 1 for Player X, -1 for Player O, 0 for ongoing/draw\n",
    "\n",
    "    def print_board(self):\n",
    "        \"\"\"Print the game board with symbols.\"\"\"\n",
    "        symbols = {0: ' ', 1: 'X', -1: 'O'}\n",
    "        # Top line for section grid\n",
    "        print(\"  | 0 1 2 | 3 4 5 | 6 7 8 \")\n",
    "        print(\"--+-------+-------+-------\")\n",
    "        for i in range(9):\n",
    "            row = self.grid[i]\n",
    "            if i % 3 == 0 and i != 0:\n",
    "                print(\"--+-------+-------+-------\")\n",
    "            \n",
    "            row_str = \" | \".join([\n",
    "                \" \".join([symbols[x] for x in row[j:j+3]]) for j in range(0, 9, 3)\n",
    "            ])\n",
    "            print(f\"{i} | {row_str}\")\n",
    "\n",
    "    def get_valid_moves(self):\n",
    "        \"\"\"\n",
    "        Returns a list of valid move indices (0-80).\n",
    "        A move is valid if the cell is empty AND it is in the active section.\n",
    "        If the active section is full or won, any non-full, non-won section is valid.\n",
    "        \"\"\"\n",
    "        valid_moves = []\n",
    "        if self.active_section == -1 or not self.is_section_playable(self.active_section):\n",
    "            # If the next section is won or drawn, the player can move anywhere\n",
    "            # on the main board where the section is not yet won or drawn.\n",
    "            for section in range(9):\n",
    "                if self.is_section_playable(section):\n",
    "                    row_start, col_start = self.SECTION_MAP[section]\n",
    "                    for r in range(3):\n",
    "                        for c in range(3):\n",
    "                            global_row, global_col = row_start + r, col_start + c\n",
    "                            if self.grid[global_row, global_col] == 0:\n",
    "                                valid_moves.append(global_row * 9 + global_col)\n",
    "        else:\n",
    "            # Player is forced to play in a specific section\n",
    "            row_start, col_start = self.SECTION_MAP[self.active_section]\n",
    "            for r in range(3):\n",
    "                for c in range(3):\n",
    "                    global_row, global_col = row_start + r, col_start + c\n",
    "                    if self.grid[global_row, global_col] == 0:\n",
    "                        valid_moves.append(global_row * 9 + global_col)\n",
    "        \n",
    "        return valid_moves\n",
    "\n",
    "    def is_section_playable(self, section_number):\n",
    "        \"\"\"Check if a section has not been won or drawn.\"\"\"\n",
    "        macro_row = section_number // 3\n",
    "        macro_col = section_number % 3\n",
    "        return self.section_grid[macro_row, macro_col] == 0\n",
    "\n",
    "    def make_move(self, index):\n",
    "        \"\"\"\n",
    "        Places a player's mark on the board.\n",
    "        Returns a tuple of (reward, done) for the RL environment.\n",
    "        \"\"\"\n",
    "        if index not in self.get_valid_moves():\n",
    "            return -10, False  # Penalize for illegal moves\n",
    "\n",
    "        row, col = index // 9, index % 9\n",
    "        self.grid[row, col] = self.current_player\n",
    "        self.last_move = index\n",
    "\n",
    "        reward = 0\n",
    "        \n",
    "        # Check the section where the move was made\n",
    "        section_number = (row // 3) * 3 + (col // 3)\n",
    "        macro_row, macro_col = section_number // 3, section_number % 3\n",
    "        \n",
    "        # Check if the section is won or drawn\n",
    "        if self.is_section_won_or_drawn(section_number):\n",
    "            if self.section_grid[macro_row, macro_col] == 0:\n",
    "                section_winner = self._check_section_winner(section_number)\n",
    "                if section_winner != 0:\n",
    "                    self.section_grid[macro_row, macro_col] = section_winner\n",
    "                    reward += 10 * self.current_player\n",
    "                else: # It's a draw\n",
    "                    self.section_grid[macro_row, macro_col] = 2 # Use 2 to denote a draw\n",
    "                    reward += 5\n",
    "\n",
    "        # Check for a final game winner\n",
    "        self.winner = self._check_game_winner()\n",
    "        if self.winner != 0:\n",
    "            self.game_over = True\n",
    "            reward = 100 * self.winner\n",
    "            \n",
    "        # Check for a draw on the main board\n",
    "        if not self.game_over and len(self.get_valid_moves()) == 0:\n",
    "            self.game_over = True\n",
    "            self.winner = 2 # Draw\n",
    "            reward = 1\n",
    "        \n",
    "        # Update the active section for the next player\n",
    "        next_section = (row % 3) * 3 + (col % 3)\n",
    "        if self.is_section_playable(next_section):\n",
    "            self.active_section = next_section\n",
    "        else:\n",
    "            self.active_section = -1  # Any section is valid\n",
    "        \n",
    "        self.current_player *= -1\n",
    "        \n",
    "        return reward, self.game_over\n",
    "\n",
    "    def _check_section_winner(self, section_number):\n",
    "        \"\"\"Check for a winner in a specific 3x3 section.\"\"\"\n",
    "        row_start, col_start = self.SECTION_MAP[section_number]\n",
    "        section = self.grid[row_start:row_start + 3, col_start:col_start + 3]\n",
    "        \n",
    "        sums = [\n",
    "            np.sum(section, axis=1), # Rows\n",
    "            np.sum(section, axis=0), # Columns\n",
    "            np.diag(section),        # Main diagonal\n",
    "            np.diag(np.fliplr(section)) # Anti-diagonal\n",
    "        ]\n",
    "        \n",
    "        for s in sums:\n",
    "            if np.any(s == 3):\n",
    "                return 1\n",
    "            if np.any(s == -3):\n",
    "                return -1\n",
    "        return 0\n",
    "\n",
    "    def is_section_won_or_drawn(self, section_number):\n",
    "        \"\"\"Checks if a section is either won, or is a draw.\"\"\"\n",
    "        # A section is won if a player has 3 in a row\n",
    "        if self._check_section_winner(section_number) != 0:\n",
    "            return True\n",
    "        # A section is a draw if all cells are filled and no one has won\n",
    "        row_start, col_start = self.SECTION_MAP[section_number]\n",
    "        section = self.grid[row_start:row_start + 3, col_start:col_start + 3]\n",
    "        if np.all(section != 0):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _check_game_winner(self):\n",
    "        \"\"\"Check for a winner on the 3x3 section grid.\"\"\"\n",
    "        sums = [\n",
    "            np.sum(self.section_grid, axis=1),\n",
    "            np.sum(self.section_grid, axis=0),\n",
    "            np.diag(self.section_grid),\n",
    "            np.diag(np.fliplr(self.section_grid))\n",
    "        ]\n",
    "        for s in sums:\n",
    "            if np.any(s == 3):\n",
    "                return 1\n",
    "            if np.any(s == -3):\n",
    "                return -1\n",
    "        return 0\n",
    "\n",
    "\n",
    "class UltimateTicTacToeEnv:\n",
    "    \"\"\"An OpenAI Gym-style environment for Ultimate Tic-Tac-Toe.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.game = UltimateTicTacToeBoard()\n",
    "        self.action_space = 81\n",
    "        self.observation_space = (9, 9)\n",
    "        self.winner = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment and returns the initial state.\"\"\"\n",
    "        self.game.reset()\n",
    "        obs = self._get_obs()\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Takes a single step in the environment.\n",
    "        Returns observation, reward, done, and info.\n",
    "        \"\"\"\n",
    "        # The reward is player-specific, so we need to track the player before the move\n",
    "        player_before_move = self.game.current_player\n",
    "        \n",
    "        # An invalid move immediately ends the game with a large negative reward\n",
    "        if action not in self.game.get_valid_moves():\n",
    "            return self._get_obs(), -100, True, {'winner': -player_before_move, 'valid': False}\n",
    "\n",
    "        reward, done = self.game.make_move(action)\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        info = {\n",
    "            'winner': self.game.winner,\n",
    "            'valid': True\n",
    "        }\n",
    "        \n",
    "        # The reward should be from the perspective of the player who just moved\n",
    "        # But the environment handles the reward based on the game state\n",
    "        if done:\n",
    "            if self.game.winner == 1:\n",
    "                final_reward = 100\n",
    "            elif self.game.winner == -1:\n",
    "                final_reward = -100\n",
    "            else: # Draw\n",
    "                final_reward = 10\n",
    "            return obs, final_reward, done, info\n",
    "            \n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Prints the current state of the board.\"\"\"\n",
    "        self.game.print_board()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns a tuple representing the current game state for the agent.\n",
    "        (main_board, section_board, valid_moves_mask)\n",
    "        \"\"\"\n",
    "        valid_moves = self.game.get_valid_moves()\n",
    "        valid_mask = np.zeros(self.action_space, dtype=bool)\n",
    "        valid_mask[valid_moves] = True\n",
    "        return (\n",
    "            np.copy(self.game.grid),\n",
    "            np.copy(self.game.section_grid),\n",
    "            valid_mask.reshape(9, 9)\n",
    "        )\n",
    "\n",
    "# Helper functions for state processing\n",
    "def convert_state_to_tensor(state, player):\n",
    "    \"\"\"\n",
    "    Converts a state tuple into a PyTorch tensor with the correct\n",
    "    channel-first format for the network.\n",
    "    \n",
    "    Args:\n",
    "        state (tuple): (main_board, section_board, valid_mask)\n",
    "        player (int): 1 or -1, the current player's turn.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (1, 3, 9, 9).\n",
    "    \"\"\"\n",
    "    board, macroboard, valid_mask = state\n",
    "    \n",
    "    # Flip the perspective so the current player is always '1'\n",
    "    # This simplifies the learning for the neural network\n",
    "    p_board = board * player\n",
    "    p_macroboard = macroboard * player\n",
    "    \n",
    "    # Create a 9x9 macroboard representation\n",
    "    expanded_macro = np.repeat(p_macroboard, 3, axis=0).repeat(3, axis=1)\n",
    "\n",
    "    stacked_state = np.stack(\n",
    "        [p_board, expanded_macro, valid_mask.astype(np.float32)],\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    tensor_state = torch.from_numpy(stacked_state).float().unsqueeze(0)\n",
    "    return tensor_state\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer for storing and sampling experiences.\"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Adds a new experience to the buffer.\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Randomly samples a batch of experiences.\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for Ultimate Tic-Tac-Toe.\"\"\"\n",
    "    def __init__(self, input_shape=(3, 9, 9), n_actions=81):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        c, h, w = input_shape\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(c, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * h * w, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# --- New MCTS Classes ---\n",
    "class MCTSNode:\n",
    "    \"\"\"A node in the Monte Carlo Tree Search tree.\"\"\"\n",
    "    def __init__(self, parent, action=None, state=None):\n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.state = state\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.total_reward = 0\n",
    "        self.is_terminal = False\n",
    "        self.untried_moves = None\n",
    "\n",
    "    def ucb1_score(self, c_param):\n",
    "        \"\"\"Calculate the UCB1 score for selection.\"\"\"\n",
    "        if self.visits == 0:\n",
    "            return float('inf') # Prioritize untried nodes\n",
    "        return (self.total_reward / self.visits) + c_param * math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"Monte Carlo Tree Search implementation.\"\"\"\n",
    "    def __init__(self, agent_network, env, c_param=2.0):\n",
    "        self.agent_network = agent_network\n",
    "        self.env = env\n",
    "        self.c_param = c_param\n",
    "        self.device = agent_network.device\n",
    "\n",
    "    def search(self, root_state, num_simulations):\n",
    "        \"\"\"Performs MCTS simulations starting from the root state.\"\"\"\n",
    "        root = MCTSNode(parent=None, action=None, state=root_state)\n",
    "        root.untried_moves = self.env.game.get_valid_moves()\n",
    "\n",
    "        for _ in range(num_simulations):\n",
    "            node = self.select(root)\n",
    "            if not node.is_terminal:\n",
    "                expanded_node = self.expand(node)\n",
    "                reward = self.simulate(expanded_node)\n",
    "                self.backpropagate(expanded_node, reward)\n",
    "\n",
    "        # After simulations, choose the best child based on visit counts\n",
    "        best_child = max(root.children, key=lambda c: c.visits)\n",
    "        return best_child.action\n",
    "\n",
    "    def select(self, node):\n",
    "        \"\"\"Selects the best child node using the UCB1 algorithm.\"\"\"\n",
    "        while node.children:\n",
    "            best_score = -float('inf')\n",
    "            best_child = None\n",
    "            for child in node.children:\n",
    "                score = child.ucb1_score(self.c_param)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_child = child\n",
    "            if best_child and best_child.untried_moves:\n",
    "                return best_child\n",
    "            if best_child:\n",
    "                node = best_child\n",
    "            else: # All children are terminal or fully expanded, pick one with max visits\n",
    "                return max(node.children, key=lambda c: c.visits)\n",
    "        return node\n",
    "\n",
    "    def expand(self, node):\n",
    "        \"\"\"Expands the tree by adding a new child node.\"\"\"\n",
    "        move = random.choice(node.untried_moves)\n",
    "        node.untried_moves.remove(move)\n",
    "\n",
    "        # Create a new environment state for the child node\n",
    "        child_env = copy.deepcopy(self.env)\n",
    "        _, _, done, _ = child_env.step(move)\n",
    "        \n",
    "        child_state = child_env._get_obs()\n",
    "        child_node = MCTSNode(parent=node, action=move, state=child_state)\n",
    "        child_node.is_terminal = done\n",
    "        if not done:\n",
    "            child_node.untried_moves = child_env.game.get_valid_moves()\n",
    "        node.children.append(child_node)\n",
    "        return child_node\n",
    "\n",
    "    def simulate(self, node):\n",
    "        \"\"\"Performs a rollout/simulation from a given node.\"\"\"\n",
    "        if node.is_terminal:\n",
    "            return 1 if self.env.game.winner == self.env.game.current_player * -1 else -1\n",
    "\n",
    "        # Use the agent's network to evaluate the current state\n",
    "        with torch.no_grad():\n",
    "            state_tensor = convert_state_to_tensor(node.state, self.env.game.current_player)\n",
    "            q_values = self.agent_network(state_tensor.to(self.device)).cpu().squeeze()\n",
    "            valid_moves_mask = np.ones(self.env.action_space, dtype=bool)\n",
    "            valid_moves = self.env.game.get_valid_moves()\n",
    "            invalid_moves = np.delete(np.arange(self.env.action_space), valid_moves)\n",
    "            q_values[invalid_moves] = -float('inf')\n",
    "            \n",
    "            # The value of the state is the maximum Q-value\n",
    "            value = q_values.max().item()\n",
    "        \n",
    "        return value\n",
    "\n",
    "    def backpropagate(self, node, reward):\n",
    "        \"\"\"Propagates the reward back up the tree.\"\"\"\n",
    "        current_node = node\n",
    "        while current_node is not None:\n",
    "            current_node.visits += 1\n",
    "            current_node.total_reward += reward\n",
    "            # The reward is inverted for the opponent's turn\n",
    "            reward *= -1\n",
    "            current_node = current_node.parent\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Learning Agent for Ultimate Tic-Tac-Toe.\n",
    "    This agent learns a policy by interacting with the game environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.n_actions = self.env.action_space\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0 # This is now for a bit of exploration in MCTS, not for action selection\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 64\n",
    "        self.learning_rate = 0.0001\n",
    "        self.target_update_frequency = 1000\n",
    "        \n",
    "        self.q_network = DQNetwork()\n",
    "        self.target_network = DQNetwork()\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(capacity=100000)\n",
    "        self.steps_done = 0\n",
    "        self.mcts = MCTS(self.q_network, env)\n",
    "\n",
    "    def get_action(self, state, player, valid_actions, num_simulations=100):\n",
    "        \"\"\"Selects an action using MCTS.\"\"\"\n",
    "        self.mcts.env = self.env # Ensure MCTS uses the current game state\n",
    "        \n",
    "        # Use MCTS to find the best action\n",
    "        action = self.mcts.search(state, num_simulations)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Performs a single learning step from a batch of experiences.\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return # Not enough experiences to train yet\n",
    "        \n",
    "        experiences = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Separate components of the experience batch\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states_t = torch.cat([convert_state_to_tensor(s, 1) for s in states]).to(self.q_network.device)\n",
    "        actions_t = torch.tensor(actions, dtype=torch.long).unsqueeze(-1).to(self.q_network.device)\n",
    "        rewards_t = torch.tensor(rewards, dtype=torch.float32).to(self.q_network.device)\n",
    "        next_states_t = torch.cat([convert_state_to_tensor(s, -1) for s in next_states]).to(self.q_network.device)\n",
    "        dones_t = torch.tensor(dones, dtype=torch.float32).to(self.q_network.device)\n",
    "        \n",
    "        # Get Q-values for the states and actions taken\n",
    "        q_values = self.q_network(states_t).gather(1, actions_t).squeeze()\n",
    "\n",
    "        # Compute target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states_t).max(1)[0]\n",
    "            target_q_values = rewards_t + self.gamma * next_q_values * (1 - dones_t)\n",
    "            \n",
    "        # Calculate loss and update the online network\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.steps_done += 1\n",
    "        \n",
    "        # Update target network\n",
    "        if self.steps_done % self.target_update_frequency == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decays epsilon for a more greedy policy over time.\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Main training loop\n",
    "def train(agent, env, num_episodes=50000):\n",
    "    logger.info(\"Starting training...\")\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Player 1's turn\n",
    "            valid_moves_p1 = env.game.get_valid_moves()\n",
    "            action_p1 = agent.get_action(state, env.game.current_player, valid_moves_p1, num_simulations=200)\n",
    "            next_state_p1, reward_p1, done, info_p1 = env.step(action_p1)\n",
    "            \n",
    "            # Store experience from P1's perspective\n",
    "            agent.replay_buffer.push(state, action_p1, reward_p1, next_state_p1, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            state = next_state_p1\n",
    "            total_reward += reward_p1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            # Player -1's turn (as an opponent to train against)\n",
    "            valid_moves_p_1 = env.game.get_valid_moves()\n",
    "            action_p_1 = agent.get_action(state, env.game.current_player, valid_moves_p_1, num_simulations=200)\n",
    "            next_state_p_1, reward_p_1, done, info_p_1 = env.step(action_p_1)\n",
    "            \n",
    "            # Store experience from P-1's perspective\n",
    "            # The reward is inverted because it's a zero-sum game\n",
    "            agent.replay_buffer.push(state, action_p_1, -reward_p_1, next_state_p_1, done)\n",
    "            agent.learn()\n",
    "            \n",
    "            state = next_state_p_1\n",
    "            total_reward += reward_p_1\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        agent.update_epsilon()\n",
    "        if (episode + 1) % 1000 == 0:\n",
    "            logger.info(f\"Episode {episode + 1}: Total Reward = {total_reward}, Epsilon = {agent.epsilon:.3f}\")\n",
    "\n",
    "    logger.info(\"Training complete.\")\n",
    "    torch.save(agent.q_network.state_dict(), \"ultimate_ttt_agent.pt\")\n",
    "    logger.info(\"Model saved as ultimate_ttt_agent.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab8399-fc64-45cb-9fcd-e6bdd5f5e4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting training...\n",
      "  0%|‚ñè                                                                           | 21/10000 [03:13<26:01:26,  9.39s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example of how to use the code\n",
    "    env = UltimateTicTacToeEnv()\n",
    "    agent = DQNAgent(env)\n",
    "\n",
    "    # Train the agent for a number of episodes\n",
    "    # Note: Training can take a long time to converge\n",
    "    # For a quick run, you can set num_simulations to a small number\n",
    "    # (e.g., 50) and num_episodes to a smaller number.\n",
    "    train(agent, env, num_episodes=10000)\n",
    "\n",
    "    # --- After training, you can test the agent ---\n",
    "    logger.info(\"\\n--- Agent vs. Human Player Test ---\")\n",
    "    agent.q_network.load_state_dict(torch.load(\"ultimate_ttt_agent.pt\"))\n",
    "    \n",
    "    # We still use MCTS for the final policy, but with more simulations\n",
    "    # to get a better move.\n",
    "    num_test_simulations = 800\n",
    "    \n",
    "    env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    while not env.game.game_over:\n",
    "        if env.game.current_player == 1: # Human player (you)\n",
    "            try:\n",
    "                move = int(input(\"Enter your move (0-80): \"))\n",
    "                if move not in env.game.get_valid_moves():\n",
    "                    logger.info(\"Invalid move. Try again.\")\n",
    "                    continue\n",
    "                _, _, _, _ = env.step(move)\n",
    "            except ValueError:\n",
    "                logger.info(\"Invalid input. Please enter a number.\")\n",
    "                continue\n",
    "        else: # Agent's turn\n",
    "            logger.info(\"Agent is thinking with MCTS...\")\n",
    "            state = env._get_obs()\n",
    "            valid_moves = env.game.get_valid_moves()\n",
    "            action = agent.get_action(state, env.game.current_player, valid_moves, num_simulations=num_test_simulations)\n",
    "            logger.info(f\"Agent plays at: {action}\")\n",
    "            _, _, _, _ = env.step(action)\n",
    "\n",
    "        env.render()\n",
    "        \n",
    "    if env.game.winner == 1:\n",
    "        logger.info(\"Congratulations! You won!\")\n",
    "    elif env.game.winner == -1:\n",
    "        logger.info(\"The agent won. Better luck next time!\")\n",
    "    else:\n",
    "        logger.info(\"It's a draw!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ea049-ca48-4b51-909a-eed98ee1932a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
