{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb4e870-cfb6-4436-a9fc-39f4e9e5144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.entities import name2codepoint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Linear, ReLU, Dropout, BatchNorm1d\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "from game import UltimateTicTacToeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab75248-ba37-4721-a321-931785dc7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-15 09:24:03,463 - DEBUG - Debug message: good for step-by-step inspection.\n",
      "2025-08-15 09:24:03,464 - INFO - Info message: general updates.\n",
      "2025-08-15 09:24:03,465 - WARNING - Warning message: something might be wrong.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)  # Capture all levels (DEBUG and above)\n",
    "\n",
    "# Clear existing handlers (important in Jupyter to avoid duplicate logs)\n",
    "logger.handlers.clear()\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.debug(\"Debug message: good for step-by-step inspection.\")\n",
    "logger.info(\"Info message: general updates.\")\n",
    "logger.warning(\"Warning message: something might be wrong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f163678f-a9da-49d0-b70b-d34702cb4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## agent\n",
    "def processObs(observation):\n",
    "    return np.array(observation[0] + observation[1] +[1 if i in observation[2] else 0 for i in range(9)]).flatten()\n",
    "\n",
    "def convert_state_to_tensor(state):\n",
    "    board, macroboard, valid_mask = state\n",
    "\n",
    "    board = board.astype(np.float32)\n",
    "    if macroboard.shape != (9,9):\n",
    "        macroboard = np.repeat(macroboard, 3, axis=0).repeat(3, axis=1).astype(np.float32)\n",
    "    valid_mask = valid_mask.astype(np.float32)\n",
    "    # print(f\"board: {board.shape}, macroboard: {macroboard.shape}, valid mask: {valid_mask.shape}\")\n",
    "    if macroboard.shape != (9,9):\n",
    "        print(macroboard)\n",
    "    \n",
    "    stacked = np.stack([board, macroboard, valid_mask], axis=-1)  # Shape: (9, 9, 3)\n",
    "    return stacked\n",
    "\n",
    "def perspective_state(state, player):\n",
    "    board, macro, valid_mask = state\n",
    "    if player == -1:\n",
    "        board = -board\n",
    "        macro = -macro\n",
    "    return board, macro, valid_mask\n",
    "\n",
    "def encode_perspective(board, player):\n",
    "    \"\"\"\n",
    "    Flip the board perspective so that 'player' is always 1.\n",
    "    \n",
    "    board: np.array shape (9,9) or (9,9,2)\n",
    "    player: 1 or -1\n",
    "    \n",
    "    returns: board with current player's perspective\n",
    "    \"\"\"\n",
    "    # If board is (9,9)\n",
    "    if board.ndim == 2:\n",
    "        return board * player  # flips sign if player == -1\n",
    "    \n",
    "    # If board is stacked (9,9,2)\n",
    "    elif board.ndim == 3:\n",
    "        # Assume first channel = current player positions, second = opponent\n",
    "        if player == 1:\n",
    "            return board.copy()\n",
    "        else:\n",
    "            return np.stack([board[:,:,1], board[:,:,0]], axis=-1)  # swap channels\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, mem_size, input_shape):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_counter = 0\n",
    "\n",
    "        self.state_memory = np.zeros((mem_size, *input_shape), dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((mem_size, *input_shape), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(mem_size, dtype=np.float32)\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = float(done)\n",
    "        self.mem_counter += 1\n",
    "\n",
    "    def sample_memory(self, batch_size):\n",
    "        max_mem = min(self.mem_counter, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        return (self.state_memory[batch],\n",
    "                self.action_memory[batch],\n",
    "                self.reward_memory[batch],\n",
    "                self.new_state_memory[batch],\n",
    "                self.terminal_memory[batch])\n",
    "\n",
    "\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 9, 9), n_actions=81, learning_rate=0.00005):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        c, h, w = input_shape[0], input_shape[1], input_shape[2]  # (channels, height, width)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * h * w, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)  # One Q-value per cell (81 actions)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if len(x.shape) == 3:  # (H, W, C)\n",
    "        #     x = x.permute(2, 0, 1).unsqueeze(0)  # → (1, C, H, W)\n",
    "        # elif len(x.shape) == 4:  # (batch, H, W, C)\n",
    "        #     x = x.permute(0, 3, 1, 2)  # → (batch, C, H, W)\n",
    "        return self.net(x)\n",
    "        \n",
    "    # def save(self, name = \"model_uttt\"):\n",
    "    #     torch.save(self,\"./models/\" + name + \".pt\")\n",
    "\n",
    "    def save(self, prefix=\"model\"):\n",
    "        # Get current time in DD_MM_YY_HH format\n",
    "        timestamp = datetime.now().strftime(\"%d_%m_%y_%H\")\n",
    "        filename = f\"./models/{prefix}_{timestamp}.pt\"\n",
    "        torch.save(self.state_dict(), filename)\n",
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self, player=1, env=UltimateTicTacToeEnv(), loading=True, name=\"\"):\n",
    "        learning_rate = 0.0015\n",
    "        gamma = 0.9\n",
    "        batch_size = 64\n",
    "        n_actions = env.action_space\n",
    "        mem_size = 10_000\n",
    "        min_memory_for_training = 1000\n",
    "        epsilon = 1\n",
    "        epsilon_dec = 0.995\n",
    "        epsilon_min = 0.05\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.n_actions = n_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.mem_size = mem_size\n",
    "        self.min_memory_for_training = min_memory_for_training\n",
    "        self.it_counter = 0\n",
    "\n",
    "        input_shape = (3, 9, 9)  # (C ,H, W,)\n",
    "        self.q = DQNetwork(input_shape, n_actions, learning_rate) # online\n",
    "        self.q_target = DQNetwork(input_shape, n_actions, learning_rate) # target\n",
    "        self.q_target.load_state_dict(self.q.state_dict())\n",
    "\n",
    "        self.target_update_freq = 1000\n",
    "        self.replay_buffer = ReplayBuffer(mem_size, input_shape)\n",
    "\n",
    "        if loading:\n",
    "            self.q.load_state_dict(torch.load(name))\n",
    "        else:\n",
    "            # self.learnNN(env)\n",
    "            pass\n",
    "\n",
    "    def getAction(self, env, observation):\n",
    "        observation = convert_state_to_tensor(observation)\n",
    "        observation = torch.from_numpy(observation).float().permute(2, 0, 1).unsqueeze(0).to(self.q.device)\n",
    "        q = self.q.forward(observation)\n",
    "        action = int(torch.argmax(q))\n",
    "    \n",
    "        # Always filter invalid moves\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if action not in valid_actions:\n",
    "            q_min = float(torch.min(q))\n",
    "            mask = np.array([i in valid_actions for i in range(env.action_space)])\n",
    "            new_q = (q.detach().cpu().numpy() - q_min + 1.) * mask\n",
    "            action = int(np.argmax(new_q))\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def pickActionMaybeRandom(self, env, observation):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return int(np.random.choice(env.get_valid_actions()))\n",
    "        else:\n",
    "            return self.getAction(env, observation)\n",
    "    \n",
    "    def pickActionMaybeMasked(self, env, observation):\n",
    "        # Now identical to pickActionMaybeRandom but with greedy fallback\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return int(np.random.choice(env.get_valid_actions()))\n",
    "        else:\n",
    "            return self.getAction(env, observation)\n",
    "\n",
    "    ## new - online and target\n",
    "    def learn(self, error):\n",
    "        if self.replay_buffer.mem_counter < self.min_memory_for_training:\n",
    "            # print(\"NO LEARNING\")\n",
    "            # print(f\"mem_counter = {self.replay_buffer.mem_counter}\\n min_memory_for_training = {self.min_memory_for_training}\")\n",
    "            return\n",
    "    \n",
    "        states, actions, rewards, new_states, dones = self.replay_buffer.sample_memory(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.q.device)\n",
    "        new_states = torch.tensor(new_states, dtype=torch.float32).to(self.q.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.q.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.q.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.q.device)\n",
    "    \n",
    "        # Compute current Q-values (using online network)\n",
    "        current_q = self.q(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute target Q-values (using target network)\n",
    "        with torch.no_grad():  # No gradient for target computation\n",
    "            next_q = self.q_target(new_states).max(1)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "    \n",
    "        # Compute loss and backpropagate\n",
    "        loss = self.q.loss(current_q, target_q)\n",
    "        self.q.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q.optimizer.step()\n",
    "    \n",
    "        # Periodically update target network\n",
    "        if self.it_counter % self.target_update_freq == 0:\n",
    "            self.q_target.load_state_dict(self.q.state_dict())\n",
    "    \n",
    "        # Adjust epsilon\n",
    "        # if error == 0:\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_dec)\n",
    "        # else:\n",
    "            # self.epsilon = min(1.0, self.epsilon / self.epsilon_dec)\n",
    "\n",
    "        # print(f\"epsilon: {self.epsilon}\")\n",
    "        self.it_counter += 1\n",
    "\n",
    "    \n",
    "    # def learn(self, error):\n",
    "    #     if self.replay_buffer.mem_counter < self.min_memory_for_training:\n",
    "    #         return\n",
    "            \n",
    "    #     states, actions, rewards, new_states, dones = self.replay_buffer.sample_memory(self.batch_size)\n",
    "    #     self.q.optimizer.zero_grad()\n",
    "        \n",
    "    #     # Convert to tensors and move to device\n",
    "    #     states_batch = torch.tensor(states, dtype=torch.float32).to(self.q.device)\n",
    "    #     new_states_batch = torch.tensor(new_states, dtype=torch.float32).to(self.q.device)\n",
    "    #     actions_batch = torch.tensor(actions, dtype=torch.long).to(self.q.device)\n",
    "    #     rewards_batch = torch.tensor(rewards, dtype=torch.float32).to(self.q.device)\n",
    "    #     dones_batch = torch.tensor(dones, dtype=torch.float32).to(self.q.device)\n",
    "\n",
    "    #     # Current Q values for chosen actions\n",
    "    #     current_q = self.q.forward(states_batch).gather(1, actions_batch.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "    #     # Target Q values\n",
    "    #     with torch.no_grad():\n",
    "    #         next_q = self.q.forward(new_states_batch).max(1)[0]\n",
    "    #         target_q = rewards_batch + (1 - dones_batch) * self.gamma * next_q\n",
    "\n",
    "    #     # Compute loss and backpropagate\n",
    "    #     loss = self.q.loss(current_q, target_q)\n",
    "    #     loss.backward()\n",
    "    #     self.q.optimizer.step()\n",
    "\n",
    "    #     # Adjust epsilon\n",
    "    #     if error == 0:\n",
    "    #         self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_dec)\n",
    "    #     else:\n",
    "    #         self.epsilon = min(1.0, self.epsilon / self.epsilon_dec)\n",
    "\n",
    "    #     self.it_counter += 1\n",
    "\n",
    "    # def learn(self, error):\n",
    "    #     if self.replay_buffer.mem_counter < self.min_memory_for_training:\n",
    "    #         return\n",
    "    #     states, actions, rewards, new_states, dones = self.replay_buffer.sample_memory(self.batch_size)\n",
    "    #     self.q.optimizer.zero_grad()\n",
    "    #     states_batch = torch.tensor(states, dtype = torch.float32).permute(2, 0, 1).unsqueeze(0).to(self.q.device)\n",
    "    #     new_states_batch = torch.tensor(new_states,dtype = torch.float32).permute(2, 0, 1).unsqueeze(0).to(self.q.device)\n",
    "        \n",
    "    #     actions_batch = torch.tensor(actions, dtype = torch.long).to(self.q.device)\n",
    "    #     rewards_batch = torch.tensor(rewards, dtype = torch.float32).to(self.q.device)\n",
    "    #     dones_batch = torch.tensor(dones, dtype = torch.float32).to(self.q.device)\n",
    "\n",
    "    #     target = rewards_batch + torch.mul(self.gamma* self.q(new_states_batch).max(axis = 1).values, (1 - dones_batch))\n",
    "    #     prediction = self.q.forward(states_batch).gather(1,actions_batch.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    #     loss = self.q.loss(prediction, target)\n",
    "    #     loss.backward()  # Compute gradients\n",
    "    #     self.q.optimizer.step()  # Backpropagate error\n",
    "\n",
    "    #     # decrease epsilon:\n",
    "    #     if error == 0:\n",
    "    #         if self.epsilon * self.epsilon_dec > self.epsilon_min:\n",
    "    #             self.epsilon *= self.epsilon_dec\n",
    "    #     else:\n",
    "    #         if self.epsilon / self.epsilon_dec <= 1:\n",
    "    #             self.epsilon /= self.epsilon_dec\n",
    "\n",
    "    #     self.it_counter += 1\n",
    "    #     return\n",
    "\n",
    "    \n",
    "    def learnNN(self, env, n_episodes=10, n_save=1000, trainingName=\"\"):\n",
    "        for ep in tqdm.tqdm(range(n_episodes)):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            current_player = 1\n",
    "            moves_sequence = []\n",
    "            last_state = {1: None, -1: None}\n",
    "            last_action = {1: None, -1: None}\n",
    "            while not done:\n",
    "                # Create player perspective state\n",
    "                p_state = perspective_state(state, current_player)\n",
    "                p_state_t = convert_state_to_tensor(p_state)\n",
    "\n",
    "                # print(f\"p_state:\\n{p_state[0]}\")\n",
    "    \n",
    "                valid_actions = env.get_valid_actions()\n",
    "                if len(valid_actions) == 0:\n",
    "                    logger.debug(\"WARNING: No valid actions found!\")\n",
    "                    logger.debug(f\"moves: {moves_sequence}, len: {len(moves_sequence)}\")\n",
    "                    \n",
    "                    break\n",
    "    \n",
    "                action = self.pickActionMaybeMasked(env, p_state) # zmiana na p_state\n",
    "                moves_sequence.append(action)\n",
    "                last_state[current_player] = p_state\n",
    "                last_action[current_player] = action\n",
    "                new_state, reward, done, error = env.step(action)\n",
    "                # logger.debug(action)\n",
    "                # Flip reward for perspective\n",
    "                reward_p = reward if current_player == 1 else -reward\n",
    "\n",
    "                # if reward_p != 0:\n",
    "                # print(reward_p)\n",
    "                \n",
    "                \n",
    "                # Store experience\n",
    "                state_s = p_state_t.transpose(2, 0, 1)  # Convert to (C, H, W)\n",
    "                state_ns = convert_state_to_tensor(perspective_state(new_state, current_player)).transpose(2, 0, 1)\n",
    "\n",
    "                # print(f\"state :\\n{state_s[0]}\")\n",
    "                # print(f\"new state :\\n{state_ns[0]}\")\n",
    "                self.replay_buffer.store_transition(\n",
    "                    state_s, action, reward_p, state_ns, done\n",
    "                )\n",
    "    \n",
    "                # Learn\n",
    "                self.learn(error)\n",
    "    \n",
    "                # Advance turn\n",
    "                state = new_state\n",
    "                current_player *= -1\n",
    "                # print(f\"-------{action}\")\n",
    "                # env.game.board.print_board()\n",
    "                \n",
    "            # logger.debug(\"restart\")\n",
    "            # Save checkpoint\n",
    "            if (ep + 1) % n_save == 0:\n",
    "                self.q.save(f\"{trainingName}_{ep+1}\")\n",
    "        \n",
    "        agent.q.save(prefix=\"ultimate_ttt_1m\")\n",
    "    \n",
    "\n",
    "    # def learnNN(self, env, masked = True, n_episodes = 10000, n_save = 1000, trainingName = \"\"):\n",
    "    #     l_epsilon = []\n",
    "    #     l_win = []\n",
    "    #     sum_win = 0\n",
    "    #     i = 1\n",
    "    #     for episode in tqdm.tqdm(range(n_episodes)):\n",
    "    #         state = env.reset()         # resetting the environment after each episode\n",
    "    #         score = 0\n",
    "    #         done = 0\n",
    "    #         while not done:               # while the episode is not over yet\n",
    "    #             action = None\n",
    "    #             if masked:\n",
    "    #                 action = self.getAction(env, state)\n",
    "    #             else:\n",
    "    #                 action = self.pickActionMaybeMasked(env,state)           # let the agent act\n",
    "    #             logging.debug(action)\n",
    "    #             new_state,reward, done, error = env.step(action) # performing the action in the environment\n",
    "    #             score += reward                            #  the total score during this round\n",
    "    #             state = np.transpose(convert_state_to_tensor(state), (2, 0, 1))\n",
    "    #             new_state = np.transpose(convert_state_to_tensor(new_state), (2, 0, 1))\n",
    "    #             self.replay_buffer.store_transition(state, action, reward,new_state, done)   # store timestep for experiene replay\n",
    "    #             # self.learn(error)                            # the agent learns after each timestep\n",
    "    #             state = new_state\n",
    "    #             # print(f\"state learnNN: {state}\")\n",
    "    #             # print(i)\n",
    "    #             i+=1\n",
    "    #         if env.game.state == 1:\n",
    "    #             sum_win +=1\n",
    "    #         elif env.game.state == -1:\n",
    "    #             sum_win -= 1\n",
    "    #         elif env.game.state == 2:\n",
    "    #             sum_win += 0\n",
    "\n",
    "    #         l_epsilon.append(self.epsilon)\n",
    "    #         l_win.append(sum_win)\n",
    "\n",
    "    #         if (episode+1) % n_save == 0:\n",
    "    #             self.q.save(trainingName + \"_\" + str(episode+1))\n",
    "\n",
    "    #     # env.close()\n",
    "    #     self.q.save(trainingName + \"_final\")\n",
    "    #     print(l_epsilon)\n",
    "    #     print(\"\\n\")\n",
    "    #     print(l_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ef0e9d-7ea1-422f-be1c-d1c84120cf7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6bc8ed7-3b18-4b2a-9edc-0f2212ede164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = UltimateTicTacToeEnv()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18f9637a-6a3b-4389-8be2-c10a7f4c7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done, info = env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "741fa026-708d-43d5-8eba-67525fd58e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 9, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation = convert_state_to_tensor(state)\n",
    "observation = torch.from_numpy(observation).float().permute(2, 0, 1).unsqueeze(0)\n",
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27092f3a-a34c-4be7-a0de-1589a989ec43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                      | 1433/70000 [1:00:59<48:38:04,  2.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m agent = DQNAgent(player=\u001b[32m1\u001b[39m, env=UltimateTicTacToeEnv(), loading=\u001b[38;5;28;01mTrue\u001b[39;00m, name = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodels/uttt_zero_5000_15_08_25_00.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearnNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m70_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingName\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muttt_zero\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 339\u001b[39m, in \u001b[36mDQNAgent.learnNN\u001b[39m\u001b[34m(self, env, n_episodes, n_save, trainingName)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28mself\u001b[39m.replay_buffer.store_transition(\n\u001b[32m    335\u001b[39m     state_s, action, reward_p, state_ns, done\n\u001b[32m    336\u001b[39m )\n\u001b[32m    338\u001b[39m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Advance turn\u001b[39;00m\n\u001b[32m    342\u001b[39m state = new_state\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 204\u001b[39m, in \u001b[36mDQNAgent.learn\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Compute target Q-values (using target network)\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():  \u001b[38;5;66;03m# No gradient for target computation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     next_q = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mq_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_states\u001b[49m\u001b[43m)\u001b[49m.max(\u001b[32m1\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m    205\u001b[39m     target_q = rewards + (\u001b[32m1\u001b[39m - dones) * \u001b[38;5;28mself\u001b[39m.gamma * next_q\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Compute loss and backpropagate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mDQNetwork.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# if len(x.shape) == 3:  # (H, W, C)\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m#     x = x.permute(2, 0, 1).unsqueeze(0)  # → (1, C, H, W)\u001b[39;00m\n\u001b[32m    104\u001b[39m     \u001b[38;5;66;03m# elif len(x.shape) == 4:  # (batch, H, W, C)\u001b[39;00m\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m#     x = x.permute(0, 3, 1, 2)  # → (batch, C, H, W)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\SGH\\Big_Data\\thesis\\Master_Thesis\\da_venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent = DQNAgent(player=1, env=UltimateTicTacToeEnv(), loading=True, name = r\"models/uttt_zero_1000_15_08_25_01.pt\")\n",
    "agent.learnNN(env, n_episodes=70_000, trainingName=\"uttt_zero\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81837571-97f1-402d-8ab2-1e09a8643091",
   "metadata": {},
   "outputs": [],
   "source": [
    "moves = [68, 35, 24, 74, 69, 27, 19, 58, 23, 61, 14, 51, 63, 47, 71, 34, 13, 48, 56, 17, 43, 40, 30, 1, 21, 73, 75, 64, 50, 80, 79, 76, 66, 28, 4, 5, 15, 36, 46, 67, 32, 6, 11, 44, 52, 26, 60, 18, 55, 22, 54, 9, 37, 41, 53, 70, 39, 29, 8, 16, 31, 3, 20, 78, 2, 42, 62, 33, 12]\n",
    "\n",
    "env = UltimateTicTacToeEnv()\n",
    "for move in moves:\n",
    "    state, reward, done, info = env.step(move)\n",
    "    print(env.get_valid_actions())\n",
    "    env.game.board.print_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442e018-49e4-425a-bf1e-f7b210771d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51e6b1ba-39f2-4a68-9bd8-a17453850110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· · x · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 25\n",
      "· · x · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · o ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [57 58 59 66 67 68 75 76 77]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· · x · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · o ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 15\n",
      "· · x · · · · · ·\n",
      "· · · · · · o · ·\n",
      "· · · · · · · o ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [27 28 29 36 37 38 45 46 47]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· · x · · · · · ·\n",
      "· · · · · · o · ·\n",
      "· · · · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 13\n",
      "· · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · · · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [30 31 32 39 40 41 48 49 50]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · · · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 46\n",
      "· · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · · · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · · · · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [57 58 66 67 68 75 76 77]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · · · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 60\n",
      "· · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · · · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x o · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 0  1  9 10 11 18 19 20]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · · · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x o · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 20\n",
      "x · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · o · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x o · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x · · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [61 62 69 70 71 78 79 80]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · o · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x o · ·\n",
      "· · · · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 64\n",
      "x · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · o · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x o · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [30 31 32 40 41 48 49 50]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · o · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · x · · · ·\n",
      "· · · · · x o · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 57\n",
      "x · x · · · · · ·\n",
      "· · · · o · o · ·\n",
      "· · o · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 1  9 10 11 18 19]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x · x · · · · · ·\n",
      "· x · · o · o · ·\n",
      "· · o · · · · o ·\n",
      "· x · · · · · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 32\n",
      "x · x · · · · · ·\n",
      "· x · · o · o · ·\n",
      "· · o · · · · o ·\n",
      "· x · · · o · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 6  7  8 16 17 24 26]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x · x · · · · · ·\n",
      "· x · · o · o · ·\n",
      "· · o · · · x o ·\n",
      "· x · · · o · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o · · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 65\n",
      "x · x · · · · · ·\n",
      "· x · · o · o · ·\n",
      "· · o · · · x o ·\n",
      "· x · · · o · · ·\n",
      "· · · x · · · · ·\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [33 34 35 42 43 44 51 52 53]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x · x · · · · · ·\n",
      "· x · · o · o · ·\n",
      "· · o · · · x o ·\n",
      "· x · · · o · · ·\n",
      "· · · x · · · · x\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 33\n",
      "x · x · · · · · ·\n",
      "· x · · o · o · ·\n",
      "· · o · · · x o ·\n",
      "· x · · · o o · ·\n",
      "· · · x · · · · x\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 1  9 11 18 19]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x · · · x o ·\n",
      "· x · · · o o · ·\n",
      "· · · x · · · · x\n",
      "· o · · x · · · ·\n",
      "· · · o · x o · ·\n",
      "· o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 54\n",
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x · · · x o ·\n",
      "· x · · · o o · ·\n",
      "· · · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · · o · x o · ·\n",
      "· o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 3  4  5  6  7  8 12 14 16 17 21 22 23 26 27 29 30 31 34 35 36 37 38 40\n",
      " 41 42 43 45 47 48 50 51 52 53 55 56 58 61 62 63 66 67 68 69 70 71 72 73\n",
      " 74 75 76 79 80]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x · · · x o ·\n",
      "· x · · · o o · ·\n",
      "· · · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · · o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 37\n",
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x · · · x o ·\n",
      "· x · · · o o · ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · · o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [30 31 40 41 48 50]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x · · · x o ·\n",
      "· x · x · o o · ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · · o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 34\n",
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x · · · x o ·\n",
      "· x · x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · · o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 3  4  5 12 14 21 22 23]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o ·\n",
      "· x · x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · · o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 56\n",
      "x x x · · · · · ·\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o ·\n",
      "· x · x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 6  7  8 16 17 26]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · · · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o ·\n",
      "· x · x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 6\n",
      "x x x · · · o · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o ·\n",
      "· x · x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x · ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 3  4  5  7 12 14 16 17 22 23 26 27 29 31 35 36 38 40 41 42 43 45 47 48\n",
      " 50 51 52 53 55 58 61 62 66 67 68 69 70 71 72 73 74 75 76 79 80]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · o · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o ·\n",
      "· x · x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o · ·\n",
      "x o o · · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 66\n",
      "x x x · · · o · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o ·\n",
      "· x · x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o · ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [27 29 36 38 45 47]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · o · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o ·\n",
      "· x x x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o · ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 26\n",
      "x x x · · · o · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o o\n",
      "· x x x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o · ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [61 62 69 70 71 80]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · · o · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o o\n",
      "· x x x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o x ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 5\n",
      "x x x · · o o · x\n",
      "x x x · o · o · ·\n",
      "x x x x · · x o o\n",
      "· x x x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o x ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 7 16 17]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x · · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x · o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o x ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 31\n",
      "x x x · · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o x ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 3  4 12 14 22 23]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o · o o · x o x ·\n",
      "x o o o · · · · ·\n",
      "· · · · · x x x ·\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 74\n",
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o o o o · x o x ·\n",
      "o o o o · · · · ·\n",
      "o o o · · x x x ·\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [62 69 70 71 80]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · · · · x\n",
      "· o · · x · · · ·\n",
      "o o o o · x x x x\n",
      "o o o o · · x x x\n",
      "o o o · · x x x x\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 41\n",
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · o · · x\n",
      "· o · · x · · · ·\n",
      "o o o o · x x x x\n",
      "o o o o · · x x x\n",
      "o o o · · x x x x\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [35 42 43 51 52 53]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · o · · x\n",
      "· o · · x · · x ·\n",
      "o o o o · x x x x\n",
      "o o o o · · x x x\n",
      "o o o · · x x x x\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 76\n",
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · o · · x\n",
      "· o · · x · · x ·\n",
      "o o o o · x x x x\n",
      "o o o o · · x x x\n",
      "o o o · o x x x x\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [58 67 68 75]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · o · · x\n",
      "· o · · x · · x ·\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 42\n",
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "· x x x o o o o ·\n",
      "· o · x · o o · x\n",
      "· o · · x · · x ·\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [27 36 38 45 47]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "x x x x o o o o ·\n",
      "x x x x · o o · x\n",
      "x x x · x · · x ·\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 53\n",
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "x x x x o o o o ·\n",
      "x x x x · o o · x\n",
      "x x x · x · · x o\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 4 12 14 22 23 35 40 43 48 50 51]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "x x x x o o o o ·\n",
      "x x x x · o o · x\n",
      "x x x · x · x x o\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "\n",
      "AI's turn (O)\n",
      "AI chooses: 35\n",
      "x x x x · o x x x\n",
      "x x x · o · x x x\n",
      "x x x x · · x x x\n",
      "x x x x o o o o o\n",
      "x x x x · o o o o\n",
      "x x x · x · o o o\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "\n",
      "Your turn (X)\n",
      "Valid moves: [ 4 12 14 22 23 40 48 50]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your move (0-80):  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x x x x x x x x x\n",
      "x x x x x x x x x\n",
      "x x x x x x x x x\n",
      "x x x x o o o o o\n",
      "x x x x · o o o o\n",
      "x x x · x · o o o\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "o o o x x x x x x\n",
      "Game over!\n",
      "You won!\n"
     ]
    }
   ],
   "source": [
    "def play_against_ai(model_path=r\"models/uttt_zero_5000_15_08_25_00.pt\"):\n",
    "    # Initialize environment and agent\n",
    "    env = UltimateTicTacToeEnv()\n",
    "    \n",
    "    # Load model with weights_only=False (only do this for trusted models)\n",
    "    agent = DQNAgent(player=1, env=env, loading=False, name=model_path)\n",
    "# Load model weights properly\n",
    "    state_dict = torch.load(model_path, weights_only=False)\n",
    "    agent.q.load_state_dict(state_dict)  # load into the policy network\n",
    "    agent.q_target.load_state_dict(agent.q.state_dict())  # sync target network\n",
    "\n",
    "    \n",
    "    # Rest of your code remains the same...\n",
    "    \n",
    "    # Set agent to evaluation mode (no exploration)\n",
    "    agent.epsilon = 0  # Always choose best action\n",
    "    \n",
    "    # Game loop\n",
    "    while True:\n",
    "        # Human's turn\n",
    "        env.render()\n",
    "        print(\"\\nYour turn (X)\")\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        print(f\"Valid moves: {valid_actions}\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                action = int(input(\"Enter your move (0-80): \"))\n",
    "                if action in valid_actions:\n",
    "                    break\n",
    "                print(\"Invalid move! Try again.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a number between 0-80\")\n",
    "        \n",
    "        # Human makes move\n",
    "        _, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "            print(\"Game over!\")\n",
    "            if env.game.state == 1:\n",
    "                print(\"You won!\")\n",
    "            elif env.game.state == -1:\n",
    "                print(\"AI won!\")\n",
    "            else:\n",
    "                print(\"It's a tie!\")\n",
    "            break\n",
    "            \n",
    "        # AI's turn\n",
    "        env.render()\n",
    "        print(\"\\nAI's turn (O)\")\n",
    "        state = env._get_obs()\n",
    "        action = agent.getAction(env, state)  # Get AI's move\n",
    "        print(f\"AI chooses: {action}\")\n",
    "        \n",
    "        # AI makes move\n",
    "        _, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            env.render()\n",
    "            print(\"Game over!\")\n",
    "            if env.game.state == 1:\n",
    "                print(\"You won!\")\n",
    "            elif env.game.state == -1:\n",
    "                print(\"AI won!\")\n",
    "            else:\n",
    "                print(\"It's a tie!\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example: play against the AI\n",
    "    play_against_ai()  # Replace with your actual model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1188070b-8f6b-498b-b711-4de6f06f4267",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'board' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mboard\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'board' is not defined"
     ]
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c988c29-2cdb-4491-a5db-d9ec3feda603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc808b-6f19-47df-be14-983fdc9488fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
